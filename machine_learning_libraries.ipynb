{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import your libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "    !pip install datasets\n",
    "    !pip install rich \n",
    "else:\n",
    "    print(\"Not running on Google Colab\")\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tweet_eval (/home/null/.cache/huggingface/datasets/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
      "100%|██████████| 3/3 [00:00<00:00, 552.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 3257\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1421\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 374\n",
      "    })\n",
      "})\n",
      "{'text': \"“Worry is a down payment on a problem you may never have'. \\xa0Joyce Meyer.  #motivation #leadership #worry\", 'label': 2}\n",
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['anger', 'joy', 'optimism', 'sadness'], id=None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "tweet_eval_emotion = load_dataset(\"tweet_eval\", \"emotion\")\n",
    "\n",
    "print(tweet_eval_emotion)\n",
    "\n",
    "print(tweet_eval_emotion[\"train\"][0])\n",
    "\n",
    "\n",
    "print( tweet_eval_emotion[\"train\"].features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to to classify images using TensorFlow Keras\n",
    "1.  Download a Cifar 10 Dataset from [here](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "2. Create Multi-Layer Perceptron (MLP) model using Tensorflow Keras\n",
    "3. Train the MLP model using the Cifar 10 Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow example\n",
    "import tensorflow as tf\n",
    "cifar_10 = tf.keras.datasets.cifar10\n",
    "(x_train, y_train), (x_test, y_test) =  cifar_10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape= (32, 32, 3)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use PyTorch to Classify the Same Dataset which is Cifar10\n",
    "1. Download the same Cifar 10 Dataset from [here](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "2. Create a Convoluted Neural Network (CNN) model using PyTorch\n",
    "3. Train the CNN model using the Cifar 10 Dataset\n",
    "    3.1 Set the Optimizer to SGD\n",
    "    3.2 Set the Loss Function to Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch example\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.flatten(),\n",
    "            nn.Linear(32*32*3, 128),\n",
    "            nn.Relu(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Train the model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD( net.parameters() , lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "\n",
    "        # ===================forward=====================\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # ===================backward====================\n",
    "        # zero the gradients before running the backaward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Backward pass to compte teh gradients of loow.wrt our learable param\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the Pararameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            ## Get the accuracy of the model     \n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn on the Iris Dataset\n",
    "1. Download the Iris Dataset\n",
    "- Iris dataset: classic ML benchmark for classifying iris species based on sepal/petal measurements\n",
    "- 150 observations of 3 iris species (50 each)\n",
    "- Goal is to train a machine learning model to accurately predict the species of an iris flower based on sepal and petal measurements\n",
    "2. Split the dataset into training and testing sets where 80% of the data is used for training and 20% for testing\n",
    "3. Train a Decision Tree Classifier on the training set\n",
    "4. Evaluate the model on the testing set and display the confusion matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn example\n",
    "from rich import print\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "print(iris.feature_names)\n",
    "X = iris.data\n",
    "print(iris.target_names)\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "## The size of the training and test sets\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "# Train the model\n",
    "dtc = DecisionTreeClassifier( max_depth=2)\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "predictions = dtc.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "score = dtc.score(X_test, y_test)\n",
    "print(score)\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions, labels=dtc.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                           display_labels=dtc.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy\n",
    "- Module to provide support for arrays, which don’t exist in base Python\n",
    "- Used by nearly every other module in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "my_array = np.array([1, 2, 3, 4, 5] , dtype = np.int8)\n",
    "print(f\"Number of dimensions: {my_array.ndim}\")\n",
    "print(f\"Shape of array: {my_array.shape}\")\n",
    "print(f\"Arary DataType: {my_array.dtype}\")\n",
    "\n",
    "my_2d_array = np.array([[1, 2, 3], [4, 5, 6]] , dtype = np.int8)\n",
    "print(f\"Number of dimensions: {my_2d_array.ndim}\")\n",
    "print(f\"Shape of array: {my_2d_array.shape}\")\n",
    "print(f\"Arary DataType: {my_2d_array.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matplotlib\n",
    "1. Mathplotlib is a plotting library for the Python programming language on the Cifar 10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib example\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a simple line plot\n",
    "x = [1,2,3,4,5]\n",
    "y = [1,4,9,16,25]\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "\n",
    "# Create a scatter plot\n",
    "x = [1,2,3,4,5]\n",
    "y = [1,4,9,16,25]\n",
    "plt.scatter(x,y)\n",
    "plt.show()\n",
    "\n",
    "# Create a bar chart\n",
    "x = [1,2,3,4,5]\n",
    "y = [1,4,9,16,25]\n",
    "plt.bar(x,y)\n",
    "plt.show()\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seaborn\n",
    "Seaborn: a data visualization library built on top of Matplotlib, providing a high-level interface for creating aesthetically pleasing and informative visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "# Query the figure's on-screen size and DPI. Note that when saving the figure to\n",
    "# a file, we need to provide a DPI for that separately.\n",
    "print('fig size: {0} DPI, size in inches {1}'.format(\n",
    "    fig.get_dpi(), fig.get_size_inches()))\n",
    "\n",
    "# Plot a scatter that persists (isn't redrawn) and the initial line.\n",
    "x = np.arange(0, 20, 0.1)\n",
    "ax.scatter(x, x + np.random.normal(0, 3.0, len(x)))\n",
    "line, = ax.plot(x, x - 5, 'r-', linewidth=2)\n",
    "\n",
    "def update(i):\n",
    "    label = 'timestep {0}'.format(i)\n",
    "    print(label)\n",
    "    # Update the line and the axes (with a new xlabel). Return a tuple of\n",
    "    # \"artists\" that have to be redrawn for this frame.\n",
    "    line.set_ydata(x - 5 + i)\n",
    "    ax.set_xlabel(label)\n",
    "    return line, ax\n",
    "\n",
    "# FuncAnimation will call the 'update' function for each frame; here\n",
    "# animating over 10 frames, with an interval of 200ms between frames.\n",
    "anim = FuncAnimation(fig, update, frames=np.arange(0, 10), interval=200)\n",
    "anim.save('line.gif', dpi=80, writer='imagemagick')\n",
    "# plt.show() will just loop the animation forever.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas example\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {'Name':['John', 'Anna', 'Peter', 'Linda'],\n",
    "        'Location':['New York', 'Paris', 'Berlin', 'London'],\n",
    "        'Age':[24, 13, 53, 33]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"The DataFrame is:\")\n",
    "print(df)\n",
    "\n",
    "# Select a column\n",
    "print(\"The element in the column 'Name' is:\")\n",
    "print(df['Name'])\n",
    "\n",
    "# Select multiple columns\n",
    "print(\"The element in the columns 'Name' and 'Location' is:\")\n",
    "print(df[['Name', 'Location']])\n",
    "\n",
    "# Select rows by index\n",
    "print(\"The element in the first row is:\")\n",
    "print(df.loc[0])\n",
    "\n",
    "# Select rows by condition\n",
    "print(\"The element in the rows where 'Age' is greater than 30 is:\")\n",
    "print(df[df['Age'] > 30])\n",
    "\n",
    "# A SQR query on the pandas DataFrame\n",
    "print(\"The element in the rows where 'Age' is greater than 30 is:\")\n",
    "print(df.query('Age > 30'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciPy\n",
    "1. Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SciPy example\n",
    "import scipy.optimize as opt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the function to be minimized\n",
    "def f(x):\n",
    "    return x**2 + 10*np.sin(x)\n",
    "## Create a seaborn plot of the function abov\n",
    "sns.lineplot(x=np.arange(-10,10,0.1), y=f(np.arange(-10,10,0.1)))\n",
    "\n",
    "\n",
    "# Call the optimize function\n",
    "result = opt.minimize(f, x0=0)\n",
    "\n",
    "# Print the results\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rich "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rich example\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "# Create a table\n",
    "table = Table(title=\"Machine Learning Libraries\")\n",
    "table.add_column(\"Name\", justify=\"center\", style=\"cyan\")\n",
    "table.add_column(\"Description\", justify=\"center\")\n",
    "\n",
    "# Add rows to the table\n",
    "table.add_row(\"TensorFlow\", \"Dataflow and differentiable programming\")\n",
    "table.add_row(\"Scikit-learn\", \"Classification, regression and clustering algorithms\")\n",
    "table.add_row(\"PyTorch\", \"Computer vision and natural language processing\")\n",
    "table.add_row(\"Numpy\", \"High-performance multidimensional array object\")\n",
    "table.add_row(\"Matplotlib\", \"Plotting library for Python\")\n",
    "table.add_row(\"Pandas\", \"Data structures and data analysis tools\")\n",
    "table.add_row(\"SciPy\", \"Algorithms and mathematical tools\")\n",
    "table.add_row(\"Rich\", \"Building command-line applications\")\n",
    "\n",
    "# Print the table\n",
    "console = Console()\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "1. XGBoost is an implementation of gradient boosted decision trees designed for speed and performance that is tabular dataset better than Deep Neural Network (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "# read data\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], test_size=.2)\n",
    "# create model instance\n",
    "bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic')\n",
    "# fit model\n",
    "bst.fit(X_train, y_train)\n",
    "# make predictions\n",
    "preds = bst.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gluon\n",
    "- MultiModalPredictor.load() used pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Never load data that could have come from an untrusted source, or that could have been tampered with. Only load data you trust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(123)\n",
    "\n",
    "from autogluon.core.utils.loaders import load_pd\n",
    "train_data = load_pd.load('https://autogluon-text.s3-accelerate.amazonaws.com/glue/sst/train.parquet')\n",
    "test_data = load_pd.load('https://autogluon-text.s3-accelerate.amazonaws.com/glue/sst/dev.parquet')\n",
    "subsample_size = 1000  # subsample data for faster demo, try setting this to larger values\n",
    "train_data = train_data.sample(n=subsample_size, random_state=0)\n",
    "train_data.head(10)\n",
    "# ====================== Text Classification with with MultoModalPredictor ======================\n",
    "from autogluon.multimodal import MultiModalPredictor\n",
    "import uuid\n",
    "model_path = f\"./tmp/{uuid.uuid4().hex}-automm_sst\"\n",
    "predictor = MultiModalPredictor(label='label', eval_metric='acc', path=model_path)\n",
    "predictor.fit(train_data, time_limit=180)\n",
    "# ====================== Evaluate the predictor ======================\n",
    "test_score = predictor.evaluate(test_data)\n",
    "print(test_score)\n",
    "test_score = predictor.evaluate(test_data, metrics=['acc', 'f1'])\n",
    "print(test_score)\n",
    "sentence1 = \"it's a charming and often affecting journey.\"\n",
    "sentence2 = \"It's slow, very, very, very slow.\"\n",
    "predictions = predictor.predict({'sentence': [sentence1, sentence2]})\n",
    "print('\"Sentence\":', sentence1, '\"Predicted Sentiment\":', predictions[0])\n",
    "print('\"Sentence\":', sentence2, '\"Predicted Sentiment\":', predictions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto ML Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1000])\n",
      "tensor([162, 166, 161, 164, 167])\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "url = 'https://datasets-server.huggingface.co/assets/imagenet-1k/--/default/test/12/image/image.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "model = timm.create_model('mobilenetv3_large_100', pretrained=True).eval()\n",
    "transform = timm.data.create_transform(\n",
    "    **timm.data.resolve_data_config( model.pretrained_cfg ))\n",
    "# == We can preapre this image for the model by passing it through the transform ==\n",
    "image_tensor = transform(image)\n",
    "print( image_tensor.shape ) \n",
    "# == Now we can pass that image ot th emodel to get the predictsions. We use unsqueeze to add a batch dimension ==\n",
    "output = model(image_tensor.unsqueeze(0))\n",
    "print( output.shape )\n",
    "## === We can use softmax to get the probabilities for each class and this leaves us with a tensor of shape ( num_classes, ) ==\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "print( probabilities.shape ) \n",
    "# == We can use the torch.topk function to get the top 5 predictions ==\n",
    "values, indices = torch.topk(probabilities, 5)\n",
    "print( indices)\n",
    "# == We can use the timm.utils.decode_predictions function to get the class names for the top 5 predictions ==\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'beagle', 'value': 0.8444967269897461},\n",
       " {'label': 'Walker_hound, Walker_foxhound', 'value': 0.03877762705087662},\n",
       " {'label': 'basset, basset_hound', 'value': 0.025460997596383095},\n",
       " {'label': 'bluetick', 'value': 0.01014547236263752},\n",
       " {'label': 'English_foxhound', 'value': 0.0071521932259202}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGENET_1k_URL = 'https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt'\n",
    "IMAGENET_1k_LABELS = requests.get(IMAGENET_1k_URL).text.strip().split('\\n')\n",
    "[{'label': IMAGENET_1k_LABELS[idx], 'value': val.item()} for val, idx in zip(values, indices)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot Classification Text Classification on 14 different Languages\n",
    "- XNLI is a subset of a few thousand examples from MNLI which has been translated into a 100 different languages (some low-ish resource). As with MNLI, the goal is to predict textual entailment (does sentence A imply/contradict/neither sentence B) and is a classification task (given two sentences, predict one of three labels).[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\")\n",
    "\n",
    "sequence_to_classify = \"Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU\"\n",
    "candidate_labels = [\"politics\", \"economy\", \"entertainment\", \"environment\"]\n",
    "output = classifier(sequence_to_classify, candidate_labels, multi_label=False)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_TEXT_SAMPLE = \"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\"\n",
    "# use bart in pytorch\n",
    "from transformers import pipeline\n",
    "'''\n",
    "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
    "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
    "'''\n",
    "default_hg_summarizer = pipeline(\"summarization\")\n",
    "print( default_hg_summarizer(SUMMARY_TEXT_SAMPLE) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune a small BERT on Tweet Eval emotion Subset dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"emotion\")\n",
    "print(dataset[\"train\"][0])\n",
    "print(dataset)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmagic/oBERT-3-upstream-pretrained-dense\" )\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"neuralmagic/oBERT-3-upstream-pretrained-dense\" , num_labels = 4)\n",
    "\n",
    "## Calling the training arguments \n",
    "from transformers import TrainingArguments , Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset = tokenized_dataset[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.16 ('ais_workshop')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5719a2572a93fd6957adec4b481afcada6f75447d61aaabf21d2456735c98a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
